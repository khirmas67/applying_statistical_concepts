{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# Introduction to Statistical Learning: Bias, Variance, and Bias-Variance Tradeoff\n\n## Q1: What is Unsupervised Learning?\nUnsupervised learning is a type of machine learning where the algorithm is trained on data that does not have labeled outputs. The goal is to uncover hidden patterns, relationships, or structures in the data.\n\n### Example: Clustering in Python using K-Means\n\n```python\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nX = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7], [8, 9]])\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\nprint(\"Cluster Labels:\", labels)\nprint(\"Cluster Centroids:\", centroids)\n```\n\n## Q2: How to Select Training and Testing Data\nTo select training and testing data, the dataset is usually split into training and testing sets.\n\n### Python example:\n\n```python\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndata = {'Feature1': [1, 2, 3, 4, 5, 6],\n        'Feature2': [5, 6, 7, 8, 9, 10],\n        'Target': [0, 1, 0, 1, 0, 1]}\ndf = pd.DataFrame(data)\n\nX = df[['Feature1', 'Feature2']]\ny = df['Target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(X_train, y_train, X_test, y_test)\n```\n\n## Q3: Bias, Variance, and Bias-Variance Tradeoff\nBias refers to the error introduced by approximating a complex real-world problem with a simplified model. Variance refers to the model's sensitivity to small changes in the training data.\n\n### Example of Bias-Variance Tradeoff\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(42)\nX = np.random.rand(100, 1) * 6 - 3\ny = 0.5 * X**3 - X + 2 + np.random.randn(100, 1) * 3\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef plot_model(degree):\n    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n    X_poly_train = poly_features.fit_transform(X_train)\n    X_poly_test = poly_features.transform(X_test)\n    \n    model = LinearRegression()\n    model.fit(X_poly_train, y_train)\n    \n    y_train_predict = model.predict(X_poly_train)\n    y_test_predict = model.predict(X_poly_test)\n    \n    train_mse = mean_squared_error(y_train, y_train_predict)\n    test_mse = mean_squared_error(y_test, y_test_predict)\n    \n    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)\n    X_plot_poly = poly_features.transform(X_plot)\n    y_plot = model.predict(X_plot_poly)\n    \n    plt.scatter(X_train, y_train)\n    plt.plot(X_plot, y_plot, color='r')\n    plt.title(f\"Degree {degree} Polynomial\\nTrain MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")\n    plt.show()\n\nplot_model(1)\nplot_model(3)\nplot_model(15)\n```\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "\n# Week 2: Linear and Polynomial Regression\n\n## 1. Linear Regression\n\nLinear regression is a supervised learning algorithm that models the relationship between a dependent variable (target) and one or more independent variables (features) using a straight line.\n\nThe general form of a linear regression model:\n\\[\ny = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \\dots + \beta_n x_n + \\epsilon\n\\]\nWhere:\n- \\(y\\) is the target variable.\n- \\(\beta_0\\) is the intercept.\n- \\(\beta_1, \\dots, \beta_n\\) are the coefficients for each feature \\(x_1, x_2, \\dots, x_n\\).\n- \\(\\epsilon\\) is the error term (noise).\n\n### Gradient Descent\nGradient descent is an optimization algorithm used to minimize the cost function by iteratively adjusting the model parameters.\n\n### Python Example of Linear Regression using `sklearn`:\n```python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nX_new = np.array([[0], [2]])\ny_predict = model.predict(X_new)\n\nplt.plot(X, y, \"b.\")\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.title(\"Linear Regression Example\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\nprint(\"Intercept:\", model.intercept_)\nprint(\"Slope:\", model.coef_)\n```\n\n## 2. Polynomial Regression\n\nPolynomial regression is an extension of linear regression where the relationship between features and the target variable is modeled as an nth-degree polynomial.\n\n### Python Example of Polynomial Regression using `sklearn`:\n```python\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge\n\nnp.random.seed(42)\nX = 6 * np.random.rand(100, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\nmodel = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n    (\"ridge_reg\", Ridge(alpha=1))\n])\n\nmodel.fit(X, y)\n\nX_new = np.linspace(-3, 3, 100).reshape(100, 1)\ny_predict = model.predict(X_new)\n\nplt.plot(X, y, \"b.\")\nplt.plot(X_new, y_predict, \"r-\", label=\"Polynomial (Degree=2) Fit\")\nplt.title(\"Polynomial Regression with Ridge Regularization\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n```\n"}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}