{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# Introduction to Statistical Learning: Bias, Variance, and Bias-Variance Tradeoff\n\n## Q1: What is Unsupervised Learning?\nUnsupervised learning is a type of machine learning where the algorithm is trained on data that does not have labeled outputs. The goal is to uncover hidden patterns, relationships, or structures in the data.\n\nKey Characteristics of Unsupervised Learning:\n- No labels or target variable.\n- Goal: To find underlying structure or distribution in the data.\n- Applications: Grouping similar items, reducing the dimensionality of the data, detecting anomalies.\n\n### Types of Unsupervised Learning\n1. Clustering\n   - Grouping similar data points into clusters based on some notion of similarity.\n   - **Algorithms**: K-means clustering, Hierarchical clustering.\n\n2. Dimensionality Reduction\n   - Reducing the number of input variables while preserving the most important information.\n   - **Algorithms**: Principal Component Analysis (PCA), t-SNE.\n\n### Example: Clustering in Python using K-Means\n\n```python\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Sample dataset\nX = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7], [8, 9]])\n\n# K-means clustering\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\nprint(\"Cluster Labels:\", labels)\nprint(\"Cluster Centroids:\", centroids)\n```\n\n## Q2: How to Select Training and Testing Data\nTo select training and testing data, the dataset is usually split into training and testing sets.\n\n### Common Techniques\n1. **Holdout Method**: A simple random split of the data into training (70-80%) and testing (20-30%).\n\n```python\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Example dataset\ndata = {'Feature1': [1, 2, 3, 4, 5, 6],\n        'Feature2': [5, 6, 7, 8, 9, 10],\n        'Target': [0, 1, 0, 1, 0, 1]}\ndf = pd.DataFrame(data)\n\n# Split data\nX = df[['Feature1', 'Feature2']]\ny = df['Target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(X_train, y_train, X_test, y_test)\n```\n\n2. **K-Fold Cross-Validation**: Splits data into K subsets and repeatedly trains and tests across folds.\n\n```python\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\ny = np.array([0, 1, 0, 1, 0, 1])\n\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n```\n\n3. **Stratified Split**: Ensures class proportions are preserved in both training and testing sets.\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n```\n\n## Q3: Bias, Variance, and Bias-Variance Tradeoff\nBias refers to the error introduced by approximating a complex real-world problem with a simplified model. Variance refers to the model's sensitivity to small changes in the training data.\n\n### Bias-Variance Tradeoff\nThe bias-variance tradeoff is the balance between underfitting (high bias) and overfitting (high variance).\n\n- High bias: Simple models (underfitting).\n- High variance: Complex models (overfitting).\n\nThe goal is to minimize total error, which is a combination of bias, variance, and irreducible error.\n\n### Example of Bias-Variance Tradeoff\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(42)\nX = np.random.rand(100, 1) * 6 - 3\ny = 0.5 * X**3 - X + 2 + np.random.randn(100, 1) * 3\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef plot_model(degree):\n    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n    X_poly_train = poly_features.fit_transform(X_train)\n    X_poly_test = poly_features.transform(X_test)\n    \n    model = LinearRegression()\n    model.fit(X_poly_train, y_train)\n    \n    y_train_predict = model.predict(X_poly_train)\n    y_test_predict = model.predict(X_poly_test)\n    \n    train_mse = mean_squared_error(y_train, y_train_predict)\n    test_mse = mean_squared_error(y_test, y_test_predict)\n    \n    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)\n    X_plot_poly = poly_features.transform(X_plot)\n    y_plot = model.predict(X_plot_poly)\n    \n    plt.scatter(X_train, y_train)\n    plt.plot(X_plot, y_plot, color='r')\n    plt.title(f\"Degree {degree} Polynomial\nTrain MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")\n    plt.show()\n\nplot_model(1)  # High bias (underfitting)\nplot_model(3)  # Balanced model\nplot_model(15)  # High variance (overfitting)\n```\n\n"}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}